{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data_lab1' created successfully\n"
     ]
    }
   ],
   "source": [
    "directory = \"data_lab1\"\n",
    "\n",
    "# Create directory for saving the data if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created successfully\")\n",
    "else:\n",
    "    print(f\"Directory '{directory}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Adjustable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# Configuration: how many of each type of node to create\n",
    "num_authors = 50         \n",
    "num_papers = 150         \n",
    "num_keywords = 40\n",
    "num_proceedings = 15        \n",
    "num_proceeding_editions = 20    \n",
    "num_venues = 15         \n",
    "num_journals = 10       \n",
    "num_volumes = 20    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we include some sample topics and words for creating fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"This paper explores the impact of machine learning algorithms on data analysis efficiency.\",\n",
    "    \"We present a novel approach for optimizing graph database queries.\",\n",
    "    \"This study analyzes the effects of large-scale distributed systems in cloud computing.\",\n",
    "    \"In this work, we investigate the security challenges in IoT networks.\",\n",
    "    \"This paper proposes a new model for natural language processing tasks.\",\n",
    "    \"The research examines the evolution of data privacy regulations worldwide.\",\n",
    "    \"An empirical study on the performance of blockchain technologies.\",\n",
    "    \"We provide a comparative analysis of various AI optimization techniques.\",\n",
    "    \"This study evaluates the scalability of real-time recommendation systems.\",\n",
    "    \"A new framework for cybersecurity threat detection is introduced.\"\n",
    "]\n",
    "\n",
    "# Components to generate unique titles\n",
    "adjectives = [\"Efficient\", \"Scalable\", \"Robust\", \"Secure\", \"Advanced\", \"Distributed\", \"Optimized\", \"Flexible\"]\n",
    "nouns = [\"Framework\", \"Model\", \"Approach\", \"Architecture\", \"Method\", \"Algorithm\", \"Technique\", \"System\"]\n",
    "fields = [\n",
    "    \"Machine Learning\",\n",
    "    \"Blockchain\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Natural Language Processing\",\n",
    "    \"Quantum Computing\",\n",
    "    \"Data Privacy\",\n",
    "    \"Graph Databases\",\n",
    "    \"Cloud Computing\",\n",
    "    \"Healthcare AI\",\n",
    "    \"IoT Networks\",\n",
    "]\n",
    "\n",
    "# Helper function to generate a unique title\n",
    "def generate_unique_title(existing_titles):\n",
    "    while True:\n",
    "        title = f\"{random.choice(adjectives)} {random.choice(nouns)} for {random.choice(fields)}\"\n",
    "        if title not in existing_titles:\n",
    "            existing_titles.add(title)\n",
    "            return title\n",
    "\n",
    "num_papers = 150\n",
    "\n",
    "existing_titles = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating the data (the `.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authors\n",
    "with open(os.path.join('data_lab1', 'authors.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'name'])  # header\n",
    "    for i in range(1, num_authors + 1):\n",
    "        writer.writerow([i, fake.name()])\n",
    "\n",
    "# Create keywords\n",
    "with open(os.path.join('data_lab1', 'keywords.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'name'])  # header\n",
    "    for i in range(1, num_keywords + 1):\n",
    "        writer.writerow([i, fake.word()])\n",
    "\n",
    "# Create papers\n",
    "with open(os.path.join('data_lab1', 'papers.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'title', 'abstract', 'doi'])  # header\n",
    "    for i in range(1, num_papers + 1):\n",
    "        title = generate_unique_title(existing_titles)\n",
    "        abstract = random.choice(topics)\n",
    "        doi = fake.uuid4()\n",
    "        writer.writerow([i, title, abstract, doi])\n",
    "\n",
    "# Create proceedings\n",
    "with open(os.path.join('data_lab1', 'proceedings.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'proceeding_name', 'proceeding_type'])  # header\n",
    "    for i in range(1, num_proceedings + 1):\n",
    "        writer.writerow([i, \n",
    "                         fake.company(),\n",
    "                         random.choice(['Conference', 'Workshop'])])\n",
    "\n",
    "# Create proceeding editions (subnodes of proceedings) WITH PARENT REFERENCE\n",
    "with open(os.path.join('data_lab1', 'proceeding_editions.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'number', 'start_date', 'end_date', 'proceeding_name'])  # header\n",
    "    \n",
    "    # We need to load proceedings data to reference parent names\n",
    "    proceedings_data = {}\n",
    "    with open(os.path.join('data_lab1', 'proceedings.csv'), 'r', newline='') as proc_file:\n",
    "        proc_reader = csv.reader(proc_file)\n",
    "        next(proc_reader)  # skip header\n",
    "        for row in proc_reader:\n",
    "            proceedings_data[int(row[0])] = row[1]  # Map ID to name\n",
    "    \n",
    "    # Now create proceeding editions with parent reference\n",
    "    proceeding_edition_to_parent = {}\n",
    "    for i in range(1, num_proceeding_editions + 1):\n",
    "        # Assign this edition to a specific proceeding\n",
    "        parent_id = random.randint(1, num_proceedings)\n",
    "        proceeding_edition_to_parent[i] = parent_id\n",
    "        \n",
    "        start = fake.date_between(start_date='-5y', end_date='today')\n",
    "        end = start + timedelta(days=3)  # conference lasts 3 days\n",
    "        writer.writerow([i, i, start, end, proceedings_data[parent_id]])\n",
    "\n",
    "# Create venues\n",
    "with open(os.path.join('data_lab1', 'venues.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'venue_name'])  # header\n",
    "    for i in range(1, num_venues + 1):\n",
    "        writer.writerow([i, fake.city()])\n",
    "\n",
    "# Create journals\n",
    "with open(os.path.join('data_lab1', 'journals.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'journal_name'])  # header\n",
    "    for i in range(1, num_journals + 1):\n",
    "        writer.writerow([i, fake.company()])\n",
    "\n",
    "# Create journal volumes (subnodes of journals) WITH PARENT REFERENCE\n",
    "with open(os.path.join('data_lab1', 'journal_volumes.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'volume', 'year', 'issue', 'journal_name'])  # header\n",
    "\n",
    "    # Load journals data to reference parent names\n",
    "    journals_data = {}\n",
    "    with open(os.path.join('data_lab1', 'journals.csv'), 'r', newline='') as journal_file:\n",
    "        journal_reader = csv.reader(journal_file)\n",
    "        next(journal_reader)  # skip header\n",
    "        for row in journal_reader:\n",
    "            journals_data[int(row[0])] = row[1]  # Map ID to name\n",
    "    \n",
    "    # Now create journal volumes with parent reference\n",
    "    volume_to_parent = {}\n",
    "    for i in range(1, num_volumes + 1):\n",
    "        # Assign this volume to a specific journal\n",
    "        parent_id = random.randint(1, num_journals)\n",
    "        volume_to_parent[i] = parent_id\n",
    "        \n",
    "        writer.writerow([i, \n",
    "                         random.randint(1, 100), \n",
    "                         random.randint(2018, 2024), \n",
    "                         random.randint(1, 4),\n",
    "                         journals_data[parent_id]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create citation relationships between papers (CITES)\n",
    "with open(os.path.join('data_lab1', 'cites.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['source_paper', 'target_paper'])  # header\n",
    "    for _ in range(300):  # <-- change from 10 to 300\n",
    "        src = random.randint(1, num_papers)\n",
    "        tgt = random.randint(1, num_papers)\n",
    "        if src != tgt:  # avoid self-citation\n",
    "            writer.writerow([src, tgt])\n",
    "\n",
    "# Create WROTE relationships (author wrote paper)\n",
    "with open(os.path.join('data_lab1', 'wrote.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['author_id', 'paper_id', 'corresponding'])\n",
    "    for _ in range(200):\n",
    "        writer.writerow([\n",
    "            random.randint(1, num_authors),\n",
    "            random.randint(1, num_papers),\n",
    "            random.choice([True, False])\n",
    "        ])\n",
    "\n",
    "# Create HAS_KEYWORD relationships (paper has keyword)\n",
    "with open(os.path.join('data_lab1', 'has_keyword.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['paper_id', 'keyword_id'])\n",
    "    for _ in range(300):\n",
    "        writer.writerow([\n",
    "            random.randint(1, num_papers),\n",
    "            random.randint(1, num_keywords)\n",
    "        ])\n",
    "\n",
    "# Create PUBLISHED_IN relationships (paper published in proceeding or journal_volume)\n",
    "with open(os.path.join('data_lab1', 'published_in.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['paper_id', 'proceeding_edition_id', 'journal_volume_id', 'date_accepted', 'pages'])\n",
    "    for _ in range(150):\n",
    "        paper_id = random.randint(1, num_papers)\n",
    "        if random.random() < 0.5:\n",
    "            proceeding_id = random.randint(1, num_proceeding_editions)\n",
    "            journal_volume_id = ''\n",
    "        else:\n",
    "            proceeding_id = ''\n",
    "            journal_volume_id = random.randint(1, num_volumes)\n",
    "        date_accepted = fake.date_between(start_date='-3y', end_date='today')\n",
    "        pages = f\"{random.randint(1, 10)}-{random.randint(11, 20)}\"\n",
    "        writer.writerow([paper_id, proceeding_id, journal_volume_id, date_accepted, pages])\n",
    "\n",
    "# Create HAS_VOLUME relationships (journal has volume) - consistent with the volume_to_parent mapping\n",
    "with open(os.path.join('data_lab1', 'has_volume.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['journal_id', 'volume_id'])\n",
    "    for volume_id, journal_id in volume_to_parent.items():\n",
    "        writer.writerow([journal_id, volume_id])\n",
    "\n",
    "# Create HAS_EDITION relationships (proceeding has edition) - consistent with the proceeding_edition_to_parent mapping\n",
    "with open(os.path.join('data_lab1', 'has_edition.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['proceeding_id', 'edition_id'])\n",
    "    for edition_id, proceeding_id in proceeding_edition_to_parent.items():\n",
    "        writer.writerow([proceeding_id, edition_id])\n",
    "\n",
    "# Create HELD_IN relationships (proceeding edition held in venue)\n",
    "with open(os.path.join('data_lab1', 'held_in.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['proceeding_id', 'venue_id'])\n",
    "    for i in range(1, num_proceeding_editions + 1):\n",
    "        writer.writerow([\n",
    "            i,\n",
    "            random.randint(1, num_venues)\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
