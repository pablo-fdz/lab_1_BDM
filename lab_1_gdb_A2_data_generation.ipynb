{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data_lab1' created successfully\n"
     ]
    }
   ],
   "source": [
    "directory = \"data_lab1\"\n",
    "\n",
    "# Create directory for saving the data if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created successfully\")\n",
    "else:\n",
    "    print(f\"Directory '{directory}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Adjustable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# Configuration: how many of each type of node to create\n",
    "num_authors = 50         \n",
    "num_papers = 150         \n",
    "num_keywords = 40\n",
    "num_proceedings = 15        \n",
    "max_num_proceeding_editions = 1000  # Overall number of editions (for different proceedings)    \n",
    "num_venues = 15         \n",
    "num_journals = 10       \n",
    "max_num_volumes = 1000  # Overall number of volumes (for different journals)    \n",
    "max_num_citations = 100\n",
    "max_num_coauthors = 10\n",
    "max_num_keywords = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we include some sample topics and words for creating fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"This paper explores the impact of machine learning algorithms on data analysis efficiency.\",\n",
    "    \"We present a novel approach for optimizing graph database queries.\",\n",
    "    \"This study analyzes the effects of large-scale distributed systems in cloud computing.\",\n",
    "    \"In this work, we investigate the security challenges in IoT networks.\",\n",
    "    \"This paper proposes a new model for natural language processing tasks.\",\n",
    "    \"The research examines the evolution of data privacy regulations worldwide.\",\n",
    "    \"An empirical study on the performance of blockchain technologies.\",\n",
    "    \"We provide a comparative analysis of various AI optimization techniques.\",\n",
    "    \"This study evaluates the scalability of real-time recommendation systems.\",\n",
    "    \"A new framework for cybersecurity threat detection is introduced.\"\n",
    "]\n",
    "\n",
    "# Components to generate unique titles\n",
    "adjectives = [\"Efficient\", \"Scalable\", \"Robust\", \"Secure\", \"Advanced\", \"Distributed\", \"Optimized\", \"Flexible\"]\n",
    "nouns = [\"Framework\", \"Model\", \"Approach\", \"Architecture\", \"Method\", \"Algorithm\", \"Technique\", \"System\"]\n",
    "fields = [\n",
    "    \"Machine Learning\",\n",
    "    \"Blockchain\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Natural Language Processing\",\n",
    "    \"Quantum Computing\",\n",
    "    \"Data Privacy\",\n",
    "    \"Graph Databases\",\n",
    "    \"Cloud Computing\",\n",
    "    \"Healthcare AI\",\n",
    "    \"IoT Networks\",\n",
    "]\n",
    "\n",
    "# Helper function to generate a unique title\n",
    "def generate_unique_title(existing_titles):\n",
    "    while True:\n",
    "        title = f\"{random.choice(adjectives)} {random.choice(nouns)} for {random.choice(fields)}\"\n",
    "        if title not in existing_titles:\n",
    "            existing_titles.add(title)\n",
    "            return title\n",
    "\n",
    "num_papers = 150\n",
    "\n",
    "existing_titles = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating the data (the `.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authors\n",
    "with open(os.path.join('data_lab1', 'authors.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'name'])  # header\n",
    "    for i in range(1, num_authors + 1):\n",
    "        writer.writerow([i, fake.name()])\n",
    "\n",
    "# Create keywords\n",
    "with open(os.path.join('data_lab1', 'keywords.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'name'])  # header\n",
    "    for i in range(1, num_keywords + 1):\n",
    "        writer.writerow([i, fake.word()])\n",
    "\n",
    "# Create papers\n",
    "with open(os.path.join('data_lab1', 'papers.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'title', 'abstract', 'doi'])  # header\n",
    "    for i in range(1, num_papers + 1):\n",
    "        title = generate_unique_title(existing_titles)\n",
    "        abstract = random.choice(topics)\n",
    "        doi = fake.uuid4()\n",
    "        writer.writerow([i, title, abstract, doi])\n",
    "\n",
    "# Create proceedings\n",
    "with open(os.path.join('data_lab1', 'proceedings.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'proceeding_name', 'proceeding_type'])  # header\n",
    "    for i in range(1, num_proceedings + 1):\n",
    "        writer.writerow([i, \n",
    "                         fake.company(),\n",
    "                         random.choice(['Conference', 'Workshop'])])\n",
    "\n",
    "# Create proceeding editions (subnodes of proceedings) WITH PARENT REFERENCE\n",
    "with open(os.path.join('data_lab1', 'proceeding_editions.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'number', 'start_date', 'end_date', 'proceeding_name'])  # header\n",
    "    \n",
    "    # We need to load proceedings data to reference parent names\n",
    "    proceedings_data = {}\n",
    "    with open(os.path.join('data_lab1', 'proceedings.csv'), 'r', newline='') as proc_file:\n",
    "        proc_reader = csv.reader(proc_file)\n",
    "        next(proc_reader)  # skip header\n",
    "        for row in proc_reader:\n",
    "            proceedings_data[int(row[0])] = row[1]  # Map ID to name\n",
    "    \n",
    "    # Now create proceeding editions with parent reference\n",
    "    proceeding_edition_to_parent = {}\n",
    "    # Ensure each proceeding has at least 1 edition\n",
    "    for proceeding_id in range(1, num_proceedings + 1):\n",
    "        # Each proceeding will have 1-20 editions\n",
    "        num_editions_per_proceeding = random.randint(1, 20)\n",
    "        \n",
    "        for edition_num in range(1, num_editions_per_proceeding + 1):\n",
    "            edition_id = len(proceeding_edition_to_parent) + 1\n",
    "            if edition_id > max_num_proceeding_editions:\n",
    "                break  # Stop if we've reached the maximum number of editions\n",
    "                \n",
    "            proceeding_edition_to_parent[edition_id] = proceeding_id\n",
    "            \n",
    "            # Make dates sequential by year for editions of the same proceeding\n",
    "            # Start from recent years and go back for older editions\n",
    "            year = max(1970, 2024 - edition_num)  # Ensure we never go below 1970\n",
    "            start = fake.date_between(\n",
    "                start_date=datetime(year, 1, 1), \n",
    "                end_date=datetime(year, 12, 31)\n",
    "            )\n",
    "            end = start + timedelta(days=3)  # conference lasts 3 days\n",
    "            \n",
    "            writer.writerow([edition_id, edition_num, start, end, proceedings_data[proceeding_id]])\n",
    "\n",
    "# Create venues\n",
    "with open(os.path.join('data_lab1', 'venues.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'venue_name'])  # header\n",
    "    for i in range(1, num_venues + 1):\n",
    "        writer.writerow([i, fake.city()])\n",
    "\n",
    "# Create journals\n",
    "with open(os.path.join('data_lab1', 'journals.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'journal_name'])  # header\n",
    "    for i in range(1, num_journals + 1):\n",
    "        writer.writerow([i, fake.company()])\n",
    "\n",
    "# Create journal volumes (subnodes of journals) WITH PARENT REFERENCE\n",
    "with open(os.path.join('data_lab1', 'journal_volumes.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'volume', 'year', 'issue', 'journal_name'])  # header\n",
    "\n",
    "    # Load journals data to reference parent names\n",
    "    journals_data = {}\n",
    "    with open(os.path.join('data_lab1', 'journals.csv'), 'r', newline='') as journal_file:\n",
    "        journal_reader = csv.reader(journal_file)\n",
    "        next(journal_reader)  # skip header\n",
    "        for row in journal_reader:\n",
    "            journals_data[int(row[0])] = row[1]  # Map ID to name\n",
    "    \n",
    "    # Now create journal volumes with parent reference\n",
    "    volume_to_parent = {}\n",
    "    # Ensure each journal has at least 1 volume\n",
    "    for journal_id in range(1, num_journals + 1):\n",
    "        # Each journal will have 1-200 volumes\n",
    "        num_volumes_per_journal = random.randint(1, 200)\n",
    "        \n",
    "        for _ in range(num_volumes_per_journal):\n",
    "            volume_id = len(volume_to_parent) + 1\n",
    "            if volume_id > max_num_volumes:\n",
    "                break  # Stop if we've reached the maximum number of volumes\n",
    "                \n",
    "            volume_to_parent[volume_id] = journal_id\n",
    "            \n",
    "            writer.writerow([volume_id, \n",
    "                            random.randint(1, 100), \n",
    "                            random.randint(2018, 2024), \n",
    "                            random.randint(1, 4),\n",
    "                            journals_data[journal_id]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create citation relationships between papers (CITES)\n",
    "with open(os.path.join('data_lab1', 'cites.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['source_paper', 'target_paper'])  # header\n",
    "    \n",
    "    # Previous code (only one citation per paper) was changed to allow multiple citations\n",
    "    # for _ in range(300):  # <-- change from 10 to 300\n",
    "    #     src = random.randint(1, num_papers)\n",
    "    #     tgt = random.randint(1, num_papers)\n",
    "    #     if src != tgt:  # avoid self-citation\n",
    "    #         writer.writerow([src, tgt])\n",
    "\n",
    "    # Track citations to avoid duplicates\n",
    "    citation_pairs = set()\n",
    "    \n",
    "    # Ensure each paper cites at least one other paper (except some very foundational papers)\n",
    "    for source_id in range(1, num_papers + 1):\n",
    "        # Decide how many papers this paper cites (0-max_num_citations)\n",
    "        # Newer papers tend to cite more papers\n",
    "        num_citations = random.randint(0 if source_id < 20 else 3, max_num_citations)\n",
    "        \n",
    "        # Find papers to cite (can't cite itself)\n",
    "        potential_targets = [p for p in range(1, num_papers + 1) if p != source_id]\n",
    "        \n",
    "        # Select random targets to cite\n",
    "        if potential_targets and num_citations > 0:\n",
    "            targets_to_cite = random.sample(potential_targets, \n",
    "                                           min(num_citations, len(potential_targets)))\n",
    "            \n",
    "            for target_id in targets_to_cite:\n",
    "                # Avoid duplicate citations\n",
    "                if (source_id, target_id) not in citation_pairs:\n",
    "                    citation_pairs.add((source_id, target_id))\n",
    "                    writer.writerow([source_id, target_id])\n",
    "\n",
    "# Create WROTE relationships (author wrote paper)\n",
    "with open(os.path.join('data_lab1', 'wrote.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['author_id', 'paper_id', 'corresponding'])\n",
    "    \n",
    "    # Previous code (only one author per paper) was changed to allow multiple authors\n",
    "    # for _ in range(200):\n",
    "    #     writer.writerow([\n",
    "    #         random.randint(1, num_authors),\n",
    "    #         random.randint(1, num_papers),\n",
    "    #         random.choice([True, False])\n",
    "    #     ])\n",
    "\n",
    "    # Ensure each paper has at least one author and some have multiple\n",
    "    paper_authors = {}  # Track authors per paper\n",
    "    \n",
    "    # First, assign at least one author to each paper\n",
    "    for paper_id in range(1, num_papers + 1):\n",
    "        author_id = random.randint(1, num_authors)\n",
    "        paper_authors[paper_id] = [author_id]\n",
    "        writer.writerow([\n",
    "            author_id,\n",
    "            paper_id,\n",
    "            True  # First author is corresponding\n",
    "        ])\n",
    "    \n",
    "    # Then add co-authors to some papers (1-max_num_coauthors additional authors)\n",
    "    for paper_id in range(1, num_papers + 1):\n",
    "        num_coauthors = random.randint(0, max_num_coauthors)  # Some papers get additional authors\n",
    "        \n",
    "        # Get authors who aren't already on this paper\n",
    "        available_authors = [a for a in range(1, num_authors + 1) \n",
    "                             if a not in paper_authors[paper_id]]\n",
    "        \n",
    "        # Choose random co-authors from available authors\n",
    "        if available_authors and num_coauthors > 0:\n",
    "            coauthors = random.sample(available_authors, \n",
    "                                      min(num_coauthors, len(available_authors)))\n",
    "            \n",
    "            for coauthor_id in coauthors:\n",
    "                paper_authors[paper_id].append(coauthor_id)\n",
    "                writer.writerow([\n",
    "                    coauthor_id,\n",
    "                    paper_id,\n",
    "                    False  # Co-authors are not corresponding\n",
    "                ])\n",
    "\n",
    "# Create HAS_KEYWORD relationships (paper has keyword)\n",
    "with open(os.path.join('data_lab1', 'has_keyword.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['paper_id', 'keyword_id'])\n",
    "    \n",
    "    # Previous code (only one keyword per paper) was changed to allow multiple keywords\n",
    "    # for _ in range(300):\n",
    "    #     writer.writerow([\n",
    "    #         random.randint(1, num_papers),\n",
    "    #         random.randint(1, num_keywords)\n",
    "    #     ])\n",
    "\n",
    "    # Track paper-keyword pairs to avoid duplicates\n",
    "    paper_keyword_pairs = set()\n",
    "    \n",
    "    # Ensure each paper has at least 2-max_num_keywords keywords\n",
    "    for paper_id in range(1, num_papers + 1):\n",
    "        # Decide how many keywords this paper has (2-max_num_keywords)\n",
    "        num_paper_keywords = random.randint(2, max_num_keywords)\n",
    "        \n",
    "        # Select random keywords\n",
    "        keywords_to_assign = random.sample(range(1, num_keywords + 1), \n",
    "                                          min(num_paper_keywords, num_keywords))\n",
    "        \n",
    "        for keyword_id in keywords_to_assign:\n",
    "            # Avoid duplicate keyword assignments\n",
    "            if (paper_id, keyword_id) not in paper_keyword_pairs:\n",
    "                paper_keyword_pairs.add((paper_id, keyword_id))\n",
    "                writer.writerow([paper_id, keyword_id])\n",
    "    \n",
    "    # Add some additional keyword relationships to create more connections\n",
    "    # (some keywords will appear in multiple papers)\n",
    "    additional_relationships = 100\n",
    "    for _ in range(additional_relationships):\n",
    "        paper_id = random.randint(1, num_papers)\n",
    "        keyword_id = random.randint(1, num_keywords)\n",
    "        \n",
    "        if (paper_id, keyword_id) not in paper_keyword_pairs:\n",
    "            paper_keyword_pairs.add((paper_id, keyword_id))\n",
    "            writer.writerow([paper_id, keyword_id])\n",
    "\n",
    "# Create PUBLISHED_IN relationships (paper published in proceeding or journal_volume)\n",
    "with open(os.path.join('data_lab1', 'published_in.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['paper_id', 'proceeding_edition_id', 'journal_volume_id', 'date_accepted', 'pages'])\n",
    "    for _ in range(150):\n",
    "        paper_id = random.randint(1, num_papers)\n",
    "        if random.random() < 0.5:\n",
    "            proceeding_id = random.randint(1, max_num_proceeding_editions)\n",
    "            journal_volume_id = ''\n",
    "        else:\n",
    "            proceeding_id = ''\n",
    "            journal_volume_id = random.randint(1, max_num_volumes)\n",
    "        date_accepted = fake.date_between(start_date='-3y', end_date='today')\n",
    "        pages = f\"{random.randint(1, 10)}-{random.randint(11, 20)}\"\n",
    "        writer.writerow([paper_id, proceeding_id, journal_volume_id, date_accepted, pages])\n",
    "\n",
    "# Create HAS_VOLUME relationships (journal has volume) - consistent with the volume_to_parent mapping\n",
    "with open(os.path.join('data_lab1', 'has_volume.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['journal_id', 'volume_id'])\n",
    "    for volume_id, journal_id in volume_to_parent.items():\n",
    "        writer.writerow([journal_id, volume_id])\n",
    "\n",
    "# Create HAS_EDITION relationships (proceeding has edition) - consistent with the proceeding_edition_to_parent mapping\n",
    "with open(os.path.join('data_lab1', 'has_edition.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['proceeding_id', 'edition_id'])\n",
    "    for edition_id, proceeding_id in proceeding_edition_to_parent.items():\n",
    "        writer.writerow([proceeding_id, edition_id])\n",
    "\n",
    "# Create HELD_IN relationships (proceeding edition held in venue)\n",
    "with open(os.path.join('data_lab1', 'held_in.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['proceeding_id', 'venue_id'])\n",
    "    for i in range(1, max_num_proceeding_editions + 1):\n",
    "        writer.writerow([\n",
    "            i,\n",
    "            random.randint(1, num_venues)\n",
    "        ])\n",
    "\n",
    "# Create REVIEWED relationships (author reviews paper)\n",
    "with open(os.path.join('data_lab1', 'reviewed.csv'), 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        'author_id', \n",
    "        'paper_id', \n",
    "        'review_date' \n",
    "        # 'recommendation'\n",
    "        ])\n",
    "    \n",
    "    # First, load the existing WROTE relationships to avoid assigning authors to review their own papers\n",
    "    author_paper_pairs = set()\n",
    "    with open(os.path.join('data_lab1', 'wrote.csv'), 'r', newline='') as wrote_file:\n",
    "        wrote_reader = csv.reader(wrote_file)\n",
    "        next(wrote_reader)  # skip header\n",
    "        for row in wrote_reader:\n",
    "            author_id = int(row[0])\n",
    "            paper_id = int(row[1])\n",
    "            author_paper_pairs.add((author_id, paper_id))\n",
    "    \n",
    "    # Assign reviewers to papers\n",
    "    for paper_id in range(1, num_papers + 1):\n",
    "        # Decide how many reviewers (1, 3, or 5 - to have majority)\n",
    "        num_reviewers = random.choice([1, 3, 5])\n",
    "        \n",
    "        # Get potential reviewers (those who didn't write this paper)\n",
    "        potential_reviewers = [author_id for author_id in range(1, num_authors + 1) \n",
    "                              if (author_id, paper_id) not in author_paper_pairs]\n",
    "        \n",
    "        # Skip if not enough potential reviewers\n",
    "        if len(potential_reviewers) < num_reviewers:\n",
    "            continue\n",
    "            \n",
    "        # Select random reviewers\n",
    "        reviewers = random.sample(potential_reviewers, num_reviewers)\n",
    "        \n",
    "        # Write review relationships\n",
    "        for reviewer_id in reviewers:\n",
    "            review_date = fake.date_between(start_date='-2y', end_date='-1m')\n",
    "            # recommendation = random.choice(['Accept', 'Minor Revision', 'Major Revision', 'Reject'])\n",
    "            writer.writerow([\n",
    "                reviewer_id, \n",
    "                paper_id, \n",
    "                review_date \n",
    "                # recommendation\n",
    "                ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
